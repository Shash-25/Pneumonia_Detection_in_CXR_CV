{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2b2f2b-7e9a-4cf7-a8bd-8d02e31ef834",
   "metadata": {},
   "source": [
    "# Model Training - CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b50ca7c-17e6-454f-996b-8b1b82acaf4d",
   "metadata": {},
   "source": [
    "#### Here we train different CNN models from scratch to detect pneumonia\n",
    "\n",
    "#### In this notebook, we do the following :-\n",
    "1. Define basic parameters for image processing like image_size and batch_size<br>\n",
    "2. Create a tensorflow data pipeline to efficiently read the images while training the model<br>\n",
    "3. Train three different <b>5-layer-deep</b> CNN model with <b>Global Max Pooling</b>, <b>Global Average Pooling</b> and <b>Flatten</b> layer respectively to see which flattening layer gives the best performance on AUC<br>\n",
    "4. Train <b>3-layer-deep</b> and <b>7-layer-deep</b> CNN model with GlobalMaxPooling layer to observe the effect of layer depth on model performance<br>\n",
    "\n",
    "<i>The performance analysis of all these trained models is done in \"d) Model Performance Analysis\" notebook</i><br>\n",
    "\n",
    "#### The data pipeline does the following :-\n",
    "1. Reads a \"batch\" of image file_paths from the metadata\n",
    "2. Read those images and processes it - resizing, casting into appropiate datatype, image_augmentation (only in training)\n",
    "4. Return the batch of processed images for feeding into the model for training or validation\n",
    "5. The data pipeline shuffles the training set while creating a batch to train the model to avoid ordering bias\n",
    "6. It also automatically determines the appropiate #parallel_threads while mapping the file path to output image to save time\n",
    "7. It prefetches the next batch while the current batch is feed into the model for training to save time\n",
    "\n",
    "\n",
    "#### Following relevant layers were used in the architecture of the model\n",
    "1. <b>Conv2D</b> layer with 3x3 filters - to apply convolution on the input image. We have kept the 'same' padding to reduce the size of the image\n",
    "2. <b>BatchNormalization</b> - to improve training using regularization and reducing internal covariate shift while training.\n",
    "3. <b>ReLU</b> - to add non-linaer activation to convoluted images for learning robust features\n",
    "4. <b>MaxPooling2D</b> to reduce the size of the image and at the same time keep prominent features intact!\n",
    "5. <b>GlobalMaxPooling2D</b> - this layer reduces the feature maps to a singular values by taking maximum of the most prominent features within those maps for feeding into fully connected layers\n",
    "6. <b>GlobalAveragePooling2D</b> - this layer reduces the feature maps to a singular values by taking the average of all values those maps for feeding into fully connected layers\n",
    "7. <b>Dropout</b> - this is used in fully connected layers to avoid overfitting by learning robust features\n",
    "\n",
    "#### Following callback methods were used while training :-\n",
    "1. <b>ReduceLROnPlateau</b> - this will reduce the learning rate by some factor if the monitored metric (val_loss) decreases twice in epochs while training. This will avoid overfitting\n",
    "2. <b>ModelCheckpoint</b> - this saves the model after each epoch. This will allow us to use any intermediate epoch model if needed\n",
    "3. <b>CSVLogger</b> - this saves the model performance metrics after each epoch in a csv file for later analysis\n",
    "4. <b>EarlyStopping</b> - this will stop training of the model if monitored metric (val_loss) decreases 3 times in a row. This will avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd118f7-fd87-420c-a57c-cfd9743a63ad",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5019db6c-1c91-4a51-a80f-2b971c7dd590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, GlobalMaxPooling2D, ReLU, GlobalAveragePooling2D\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, CSVLogger, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9e703-0dc9-4186-90f9-099782227fd9",
   "metadata": {},
   "source": [
    "# Parameters for Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bace6957-73b1-4a4a-8a11-3b240db0f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size for training the model will be 256 x 256\n"
     ]
    }
   ],
   "source": [
    "downsize_ratio = 4\n",
    "IMG_SIZE = int(1024 / downsize_ratio)\n",
    "batch_size = 12\n",
    "print(f'Image size for training the model will be {IMG_SIZE} x {IMG_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd274b6-1125-4df6-9ea6-b740f9d2fcb0",
   "metadata": {},
   "source": [
    "Experiment with other image sizes like (512 x 512) and (341 x 341) was done. In that case, the training only gets computationally expensive while not leading to significant increment in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acec0f5-3067-44c6-b6c3-d712c7934a19",
   "metadata": {},
   "source": [
    "# Crate Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba073b3d-b9d2-4776-a158-022128025394",
   "metadata": {},
   "source": [
    "### Prepare metadata for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea4050a-14f5-4eb2-9593-53d39104d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25727, 12)\n",
      "(2250, 12)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('Data/pneumonia/train_metadata.csv')\n",
    "train_df['file_path'] = 'Data/pneumonia/Training/Images/' + train_df['patientId'] + '.png'\n",
    "\n",
    "train_data = list(zip(train_df['file_path'], train_df['Target']))\n",
    "train_paths, train_target = zip(*train_data)\n",
    "\n",
    "validation_df = pd.read_csv('Data/pneumonia/val_metadata.csv')\n",
    "validation_df['file_path'] = 'Data/pneumonia/Training/Images/' + validation_df['patientId'] + '.png'\n",
    "\n",
    "validation_data = list(zip(validation_df['file_path'], validation_df['Target']))\n",
    "validation_paths, validation_target = zip(*validation_data)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(validation_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8303f86-df41-43dc-a14f-c0c985d01f42",
   "metadata": {},
   "source": [
    "### Mapping Function to Process Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722e83a-5ed3-4f6b-8a88-db7a33666f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation block\n",
    "from tensorflow.keras.layers import RandomFlip\n",
    "\n",
    "augmentation_block = tf.keras.Sequential([\n",
    "    RandomFlip(\"vertical\"),\n",
    "], name=\"data_augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9f8613-caf2-4223-b232-e7d43ce002ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_train(image_path, target):\n",
    "\n",
    "    #read file\n",
    "    image = tf.io.read_file(image_path)\n",
    "\n",
    "    #process image\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method = 'bilinear')\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    #augmentation for training\n",
    "    image = augmentation_block(image, training=True)\n",
    "    \n",
    "    return image, target\n",
    "\n",
    "def load_and_process_validation(image_path, target):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method = 'bilinear')\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e51cb7-3110-45c8-86fb-e7e98797a6ed",
   "metadata": {},
   "source": [
    "### Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254bc574-3740-47ea-88ea-7a66d114add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((list(train_paths), list(train_target)))\n",
    "train_ds = train_ds.map(load_and_process_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((list(validation_paths),  list(validation_target)))\n",
    "val_ds = val_ds.map(load_and_process_validation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea22b3-e7c6-4a8a-a0d6-3c373f3c7267",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a6710-83c4-43b3-a26b-52c8cf816226",
   "metadata": {},
   "source": [
    "# Train 5 Layer CNN Model with different flattening layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f40178-94bb-4f79-8d03-08a0690422cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Layers\n",
    "def convolutional_block_5layer(inputs):\n",
    "    \n",
    "    #1\n",
    "    x = Conv2D(60, (3,3), padding = 'same')(inputs) #254 x 254 x 60 \n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #127 x 127 x 60\n",
    "    \n",
    "    #2\n",
    "    x = Conv2D(120, (3,3), padding = 'same')(x) #125 x 125 x 120\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #62 x 62 x 120\n",
    "\n",
    "    #3\n",
    "    x = Conv2D(240, (3,3), padding = 'same')(x) #60 x 60 x 240\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #30 x 30 x 240\n",
    "\n",
    "    #4\n",
    "    x = Conv2D(480, (3,3),  padding = 'same')(x) #28 x 28 x 480\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #14 x 14 x 480\n",
    "\n",
    "    #5\n",
    "    x = Conv2D(960, (3,3),  padding = 'same')(x) #12 x 12 x 960\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((3,3))(x) #6 x 6 x 240\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e4e574-192c-4e55-aa27-17cd528f1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_block_5layer(x):\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(200)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(50)(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(1, activation = 'sigmoid', name = 'classification')(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98fa1f-a295-484d-b9ec-71cf31854195",
   "metadata": {},
   "source": [
    "### CNN with Max Global Pooling as flattening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817510a2-e7df-4e85-9f0c-2ed91c351a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (IMG_SIZE,IMG_SIZE,1))\n",
    "convolutional_block_x = convolutional_block_5layer(inputs)\n",
    "fc_block = GlobalMaxPooling2D()(convolutional_block_x) #Global Max Pooling\n",
    "classification_block_maxpool = classification_block_5layer(fc_block)\n",
    "l5_maxglobalpool_model = Model(inputs = inputs, outputs = classification_block_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f7aa7-60aa-4d83-a399-a3adf35ced90",
   "metadata": {},
   "source": [
    "### CNN with Average Global Pooling as flattening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979594ac-5a17-4aa8-99cb-c02f878e4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (IMG_SIZE,IMG_SIZE,1))\n",
    "convolutional_block_x = convolutional_block_5layer(inputs)\n",
    "fc_block = GlobalAveragePooling2D()(convolutional_block_x) #Average Global Pooling\n",
    "classification_block_avgpool = classification_block_5layer(fc_block)\n",
    "l5_avgglobalpool_model = Model(inputs = inputs, outputs = classification_block_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4a939-43fd-46cf-8729-e1099cec5c8d",
   "metadata": {},
   "source": [
    "### CNN with Flatten Layer as flattening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d265ef8c-ed5e-477d-b6ab-5a0610566f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (IMG_SIZE,IMG_SIZE,1))\n",
    "convolutional_block_x = convolutional_block_5layer(inputs)\n",
    "fc_block = Flatten()(convolutional_block_x) # Flatten Layer\n",
    "classification_block_flatten = classification_block_5layer(fc_block)\n",
    "l5_flatten_model = Model(inputs = inputs, outputs = classification_block_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf4adc4-bd1d-45f1-be3f-9b293bb2c4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l5_maxglobalpool_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f8b0e-2280-4f18-8f1c-6c802015d254",
   "metadata": {},
   "source": [
    "###  Compile and Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80791245-2b7b-4f83-8502-a6ea99853554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile_and_fit(model,model_name):\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \\\n",
    "                  loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC(name='auc')])\n",
    "\n",
    "    os.makedirs(f\"final_models/{model_name}\", exist_ok=True)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=2, min_lr=1e-6,verbose=1)\n",
    "    \n",
    "    checkpoint_cb = ModelCheckpoint(filepath=f\"final_models/{model_name}/{model_name}_{{epoch:02d}}.keras\",save_freq='epoch', \\\n",
    "                                    save_weights_only=True,save_best_only=False,verbose=1)\n",
    "    \n",
    "    csv_logger = CSVLogger(f\"final_models//{model_name}/{model_name}_log.csv\", append=True)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss',patience=3,mode='min', verbose = 1)\n",
    "    \n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=20, verbose = 1, \\\n",
    "            callbacks=[checkpoint_cb, reduce_lr,csv_logger, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9a816d-5ff2-4d28-8c79-18d543d4c939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.7398 - precision: 0.6171 - recall: 0.4773 - auc: 0.7643\n",
      "Epoch 1: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_01.keras\n",
      "2144/2144 [==============================] - 404s 185ms/step - loss: 0.5446 - accuracy: 0.7398 - precision: 0.6171 - recall: 0.4773 - auc: 0.7643 - val_loss: 0.4504 - val_accuracy: 0.8053 - val_precision: 0.7007 - val_recall: 0.6671 - val_auc: 0.8519 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.7805 - precision: 0.6826 - recall: 0.5780 - auc: 0.8263\n",
      "Epoch 2: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_02.keras\n",
      "2144/2144 [==============================] - 396s 185ms/step - loss: 0.4686 - accuracy: 0.7805 - precision: 0.6826 - recall: 0.5780 - auc: 0.8263 - val_loss: 0.4341 - val_accuracy: 0.8187 - val_precision: 0.7385 - val_recall: 0.6573 - val_auc: 0.8695 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4483 - accuracy: 0.7895 - precision: 0.6925 - recall: 0.6071 - auc: 0.8443\n",
      "Epoch 3: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_03.keras\n",
      "2144/2144 [==============================] - 397s 185ms/step - loss: 0.4483 - accuracy: 0.7895 - precision: 0.6925 - recall: 0.6071 - auc: 0.8443 - val_loss: 0.4120 - val_accuracy: 0.8213 - val_precision: 0.7546 - val_recall: 0.6417 - val_auc: 0.8763 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.8009 - precision: 0.7110 - recall: 0.6295 - auc: 0.8593\n",
      "Epoch 4: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_04.keras\n",
      "2144/2144 [==============================] - 398s 186ms/step - loss: 0.4292 - accuracy: 0.8009 - precision: 0.7110 - recall: 0.6295 - auc: 0.8593 - val_loss: 0.4031 - val_accuracy: 0.8267 - val_precision: 0.7164 - val_recall: 0.7447 - val_auc: 0.8810 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4179 - accuracy: 0.8060 - precision: 0.7181 - recall: 0.6413 - auc: 0.8674\n",
      "Epoch 5: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_05.keras\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.4179 - accuracy: 0.8060 - precision: 0.7181 - recall: 0.6413 - auc: 0.8674 - val_loss: 0.4003 - val_accuracy: 0.8218 - val_precision: 0.6897 - val_recall: 0.7898 - val_auc: 0.8892 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8167 - precision: 0.7362 - recall: 0.6592 - auc: 0.8796\n",
      "Epoch 6: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_06.keras\n",
      "2144/2144 [==============================] - 398s 186ms/step - loss: 0.3997 - accuracy: 0.8167 - precision: 0.7362 - recall: 0.6592 - auc: 0.8796 - val_loss: 0.3873 - val_accuracy: 0.8262 - val_precision: 0.7615 - val_recall: 0.6530 - val_auc: 0.8895 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.3850 - accuracy: 0.8249 - precision: 0.7463 - recall: 0.6802 - auc: 0.8894\n",
      "Epoch 7: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_07.keras\n",
      "2144/2144 [==============================] - 398s 186ms/step - loss: 0.3850 - accuracy: 0.8249 - precision: 0.7463 - recall: 0.6802 - auc: 0.8894 - val_loss: 0.3792 - val_accuracy: 0.8271 - val_precision: 0.7156 - val_recall: 0.7489 - val_auc: 0.8957 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.3672 - accuracy: 0.8350 - precision: 0.7574 - recall: 0.7072 - auc: 0.8999\n",
      "Epoch 8: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_08.keras\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.3672 - accuracy: 0.8350 - precision: 0.7574 - recall: 0.7072 - auc: 0.8999 - val_loss: 0.3759 - val_accuracy: 0.8267 - val_precision: 0.7194 - val_recall: 0.7377 - val_auc: 0.8967 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.8458 - precision: 0.7730 - recall: 0.7286 - auc: 0.9136\n",
      "Epoch 9: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_09.keras\n",
      "2144/2144 [==============================] - 398s 186ms/step - loss: 0.3428 - accuracy: 0.8458 - precision: 0.7730 - recall: 0.7286 - auc: 0.9136 - val_loss: 0.3807 - val_accuracy: 0.8276 - val_precision: 0.8216 - val_recall: 0.5783 - val_auc: 0.9022 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8652 - precision: 0.8028 - recall: 0.7634 - auc: 0.9311\n",
      "Epoch 10: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_10.keras\n",
      "2144/2144 [==============================] - 400s 186ms/step - loss: 0.3079 - accuracy: 0.8652 - precision: 0.8028 - recall: 0.7634 - auc: 0.9311 - val_loss: 0.3721 - val_accuracy: 0.8307 - val_precision: 0.7020 - val_recall: 0.8039 - val_auc: 0.9043 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.8851 - precision: 0.8343 - recall: 0.7967 - auc: 0.9471\n",
      "Epoch 11: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_11.keras\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.2698 - accuracy: 0.8851 - precision: 0.8343 - recall: 0.7967 - auc: 0.9471 - val_loss: 0.3979 - val_accuracy: 0.8387 - val_precision: 0.8289 - val_recall: 0.6150 - val_auc: 0.9004 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9102 - precision: 0.8698 - recall: 0.8437 - auc: 0.9645\n",
      "Epoch 12: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_12.keras\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.2216 - accuracy: 0.9102 - precision: 0.8698 - recall: 0.8437 - auc: 0.9645 - val_loss: 0.3704 - val_accuracy: 0.8573 - val_precision: 0.7904 - val_recall: 0.7447 - val_auc: 0.9077 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9309 - precision: 0.9025 - recall: 0.8773 - auc: 0.9777\n",
      "Epoch 13: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_13.keras\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.1744 - accuracy: 0.9309 - precision: 0.9025 - recall: 0.8773 - auc: 0.9777 - val_loss: 0.4672 - val_accuracy: 0.8524 - val_precision: 0.8126 - val_recall: 0.6911 - val_auc: 0.9003 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9449 - precision: 0.9186 - recall: 0.9070 - auc: 0.9857\n",
      "Epoch 14: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_14.keras\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.1387 - accuracy: 0.9449 - precision: 0.9186 - recall: 0.9070 - auc: 0.9857 - val_loss: 0.4627 - val_accuracy: 0.8569 - val_precision: 0.8019 - val_recall: 0.7250 - val_auc: 0.9067 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9715 - precision: 0.9594 - recall: 0.9506 - auc: 0.9956\n",
      "Epoch 15: saving model to final_models/L5_max_pool_v2\\L5_max_pool_v2_15.keras\n",
      "2144/2144 [==============================] - 399s 186ms/step - loss: 0.0777 - accuracy: 0.9715 - precision: 0.9594 - recall: 0.9506 - auc: 0.9956 - val_loss: 0.4711 - val_accuracy: 0.8702 - val_precision: 0.7924 - val_recall: 0.7969 - val_auc: 0.9173 - lr: 1.0000e-05\n",
      "Epoch 15: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x297c965f310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_compile_and_fit(l5_flatten_model,'l5_flatten')\n",
    "# model_compile_and_fit(l5_avgglobalpool_model,'l5_avgglobalpool')\n",
    "model_compile_and_fit(l5_maxglobalpool_model,'L5_max_pool_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c0355-4f5f-4ee1-956c-4167b64e89d2",
   "metadata": {},
   "source": [
    "# Train 3 Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e61cae4-2cc8-40f1-9df4-25ce4d538866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Layers\n",
    "def CNN_3layer(inputs):\n",
    "    #1\n",
    "    x = Conv2D(60, (3,3), padding = 'same')(inputs) #254 x 254 x 30 \n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((3,3))(x) #84 x 84 x 60\n",
    "    \n",
    "    #2\n",
    "    x = Conv2D(120, (3,3), padding = 'same')(x) #82 x 82 x 60\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((3,3))(x) #27 x 27 x 60\n",
    "\n",
    "    #3\n",
    "    x = Conv2D(240, (3,3), padding = 'same')(x) #25 x 25 x 240\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = GlobalMaxPooling2D()(x) #Global Max Pooling\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(100)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(50)(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(1, activation = 'sigmoid', name = 'classification')(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55f07e06-d23d-4a07-98a9-7b36c7591a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (IMG_SIZE,IMG_SIZE,1))\n",
    "output_CNN_3layer = CNN_3layer(inputs)\n",
    "l3_maxpool_model = Model(inputs = inputs, outputs = output_CNN_3layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6662e06f-8082-462a-8e52-a00125925fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.6832 - precision: 0.5027 - recall: 0.2891 - auc: 0.6620\n",
      "Epoch 1: saving model to final_models/L3_max_pool\\L3_max_pool_01.keras\n",
      "2144/2144 [==============================] - 176s 79ms/step - loss: 0.6195 - accuracy: 0.6832 - precision: 0.5027 - recall: 0.2891 - auc: 0.6620 - val_loss: 0.5773 - val_accuracy: 0.7142 - val_precision: 0.5427 - val_recall: 0.5910 - val_auc: 0.7532 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.7214 - precision: 0.5908 - recall: 0.4007 - auc: 0.7498\n",
      "Epoch 2: saving model to final_models/L3_max_pool\\L3_max_pool_02.keras\n",
      "2144/2144 [==============================] - 168s 78ms/step - loss: 0.5404 - accuracy: 0.7214 - precision: 0.5908 - recall: 0.4007 - auc: 0.7498 - val_loss: 0.5721 - val_accuracy: 0.7253 - val_precision: 0.5553 - val_recall: 0.6446 - val_auc: 0.7649 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.5113 - accuracy: 0.7512 - precision: 0.6380 - recall: 0.5014 - auc: 0.7849\n",
      "Epoch 3: saving model to final_models/L3_max_pool\\L3_max_pool_03.keras\n",
      "2144/2144 [==============================] - 169s 79ms/step - loss: 0.5113 - accuracy: 0.7512 - precision: 0.6380 - recall: 0.5014 - auc: 0.7849 - val_loss: 0.6244 - val_accuracy: 0.6800 - val_precision: 0.4953 - val_recall: 0.8209 - val_auc: 0.7646 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4945 - accuracy: 0.7610 - precision: 0.6481 - recall: 0.5423 - auc: 0.8020\n",
      "Epoch 4: saving model to final_models/L3_max_pool\\L3_max_pool_04.keras\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "2144/2144 [==============================] - 169s 79ms/step - loss: 0.4945 - accuracy: 0.7610 - precision: 0.6481 - recall: 0.5423 - auc: 0.8020 - val_loss: 0.6641 - val_accuracy: 0.6000 - val_precision: 0.4338 - val_recall: 0.8829 - val_auc: 0.7670 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4800 - accuracy: 0.7738 - precision: 0.6690 - recall: 0.5703 - auc: 0.8157\n",
      "Epoch 5: saving model to final_models/L3_max_pool\\L3_max_pool_05.keras\n",
      "2144/2144 [==============================] - 169s 79ms/step - loss: 0.4800 - accuracy: 0.7738 - precision: 0.6690 - recall: 0.5703 - auc: 0.8157 - val_loss: 0.7133 - val_accuracy: 0.5396 - val_precision: 0.3995 - val_recall: 0.9168 - val_auc: 0.7670 - lr: 1.0000e-05\n",
      "Epoch 5: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da71228850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_compile_and_fit(l3_maxpool_model,'L3_max_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20937741-7680-408b-9dc1-f581dca397f8",
   "metadata": {},
   "source": [
    "### 7 Layer CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5958dd7a-b5d9-4cb5-b5ac-b390ddb9c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Layers\n",
    "def CNN_7layer(inputs):\n",
    "    #1\n",
    "    x = Conv2D(60, (3,3), padding = 'same')(inputs) #254 x 254 x 30 \n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #127 x 127 x 30\n",
    "    \n",
    "    #2\n",
    "    x = Conv2D(120, (3,3), padding = 'same')(x) #125 x 125 x 60\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #62 x 62 x 60\n",
    "\n",
    "    #3\n",
    "    x = Conv2D(240, (3,3), padding = 'same')(x) #60 x 60 x 120\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #30 x 30 x 120\n",
    "\n",
    "    #4\n",
    "    x = Conv2D(480, (3,3),  padding = 'same')(x) #28 x 28 x 240\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #14 x 14 x 240\n",
    "\n",
    "    #5\n",
    "    x = Conv2D(960, (3,3),  padding = 'same')(x) #12 x 12 x 960\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x) #6 x 6 x 240\n",
    "\n",
    "    #6\n",
    "    x = Conv2D(960, (3,3),  padding = 'same')(x) #10 x 10 x 1940\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((3,3))(x) #5 x 5 x 240\n",
    "\n",
    "    #7\n",
    "    x = Conv2D(960, (3,3),  padding = 'same')(x) #3 x 3 x 3380\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = GlobalMaxPooling2D()(x) #Global Max Pooling\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(200)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(50)(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Dense(1, activation = 'sigmoid', name = 'classification')(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a198c3-1b6e-4e2d-b5fd-0c080c3af6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (IMG_SIZE,IMG_SIZE,1))\n",
    "output_CNN_7layer = CNN_7layer(inputs)\n",
    "l7_maxpool_model = Model(inputs = inputs, outputs = output_CNN_7layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622e2602-44f2-4c1c-94d4-952fabd181e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4882 - accuracy: 0.7669 - precision: 0.6582 - recall: 0.5543 - auc: 0.8114\n",
      "Epoch 1: saving model to final_models/L7_max_pool\\L7_max_pool_01.keras\n",
      "2144/2144 [==============================] - 544s 250ms/step - loss: 0.4882 - accuracy: 0.7669 - precision: 0.6582 - recall: 0.5543 - auc: 0.8114 - val_loss: 0.5151 - val_accuracy: 0.7511 - val_precision: 0.7278 - val_recall: 0.3357 - val_auc: 0.7850 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.7964 - precision: 0.7017 - recall: 0.6248 - auc: 0.8479\n",
      "Epoch 2: saving model to final_models/L7_max_pool\\L7_max_pool_02.keras\n",
      "2144/2144 [==============================] - 533s 248ms/step - loss: 0.4440 - accuracy: 0.7964 - precision: 0.7017 - recall: 0.6248 - auc: 0.8479 - val_loss: 0.4951 - val_accuracy: 0.7613 - val_precision: 0.6340 - val_recall: 0.5740 - val_auc: 0.8072 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8071 - precision: 0.7211 - recall: 0.6407 - auc: 0.8605\n",
      "Epoch 3: saving model to final_models/L7_max_pool\\L7_max_pool_03.keras\n",
      "2144/2144 [==============================] - 533s 248ms/step - loss: 0.4277 - accuracy: 0.8071 - precision: 0.7211 - recall: 0.6407 - auc: 0.8605 - val_loss: 0.5628 - val_accuracy: 0.7324 - val_precision: 0.5529 - val_recall: 0.7884 - val_auc: 0.8082 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.8136 - precision: 0.7324 - recall: 0.6515 - auc: 0.8740\n",
      "Epoch 4: saving model to final_models/L7_max_pool\\L7_max_pool_04.keras\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "2144/2144 [==============================] - 534s 249ms/step - loss: 0.4088 - accuracy: 0.8136 - precision: 0.7324 - recall: 0.6515 - auc: 0.8740 - val_loss: 0.7576 - val_accuracy: 0.5080 - val_precision: 0.3858 - val_recall: 0.9478 - val_auc: 0.7902 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2144/2144 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.8352 - precision: 0.7546 - recall: 0.7131 - auc: 0.9005\n",
      "Epoch 5: saving model to final_models/L7_max_pool\\L7_max_pool_05.keras\n",
      "2144/2144 [==============================] - 533s 249ms/step - loss: 0.3666 - accuracy: 0.8352 - precision: 0.7546 - recall: 0.7131 - auc: 0.9005 - val_loss: 0.5505 - val_accuracy: 0.7236 - val_precision: 0.5432 - val_recall: 0.7715 - val_auc: 0.8069 - lr: 1.0000e-05\n",
      "Epoch 5: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15db9fa0820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_compile_and_fit(l7_maxpool_model,'L7_max_pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22efcdb-cbb1-4fee-8c10-b008c9bd48db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF GPU)",
   "language": "python",
   "name": "tf_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
